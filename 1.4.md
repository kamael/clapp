### 1.4.1 参数 vs. 无参数模型

本书中，我们将使用 $p(y|x)$ 或 $p(x)$ 这样形式的概率模型，取决于是有监督学习还是无监督学习。

有很多方法可用于定义这类模型。其中有一个最重要的区别：是否有参数(parametric model or non-parametric model)，下面将进行解释。

### 1.4.2 简单的无参模型：K邻聚类

K邻聚类(K nearest neighbor (KNN) classifier)表示对于输入$x$，在离他最近的点中，哪一类的点最多。

$$p(y=c|x, D, K) = \frac{1}{K}\sum_{i\in N_K(x, D)}{\prod(y_i=c)}$$

其中$N_K(x, D)$表示在D上，离$x$第$K$近的点集；$K$值由模型指定，$\prod(e)$ 为如下指示函数：

$$
y = \left\{ \begin{array}{ll}
1 & \textrm{if e is true}\\
0 & \textrm{if e is false}\\
\end{array} \right.
$$

这是一个基于内存或实例学习(memory-based learning or instance-based learning)的例子。最常用的距离测量方式为欧几里得距离。

当 $K=1$ 的时候，其将退化为沃罗诺伊镶嵌(Voronoi tessellation)，距离输入点最近的点的标签即为该点的标签。

如果数据集足够好，当 $N\to\infty$ 时，K邻聚类可以得到很好的结果。

但是K邻聚类不适合处理高纬问题，纬度灾难(Curse of dimensionality
)问题会导致维度上升时，样本空间过小，无法满足分类的需求

### 1.4.4 参数模型

一个解决维度灾难的办法是假定数据分布的规律，这些假定称之为归纳偏置(inductive bias)，常常以参数模型(parametric model)的形式出现。后文将深入解释。

### 1.4.5 线性回归

一个用的很广的模型称为线性回归(linear regression)

$$y(x) = w^T + \epsilon + \sum_{j=1}^{D}w_jx_j + \epsilon $$

$w^T$表示权重$w$(weight vector)和输入向量$x$(input vector)的标量积(scalar product)，$\epsilon$表示线性预测值和真实值之间的残差(residual error)

我们通常假定$\epsilon$符合高斯或平凡(Gaussian or normal)分布，即 $\epsilon \sim \mathcal{N}(\mu, \delta^2)$，其中$\mu$为均值(mean)，$\delta^2$为方差(variance)。我们将其画出，将得到一个著名的钟形曲线(bell curve)

为了更加紧密的反应线性回归和高斯分布之间的关系，我们用如下方式表示这个模型

$$p(y|x, \theta) = \mathcal{N}(y|\mu(x), \delta^2(x))$$

这清楚的反应该模型是条件概率密度模型

最简单的场景下，我们假定$\mu$是x的线性函数，即 $\mu = w^Tx$；假定噪声是常量，即 $\delta^2(x) = \delta^2$。在该场景下，$\theta = (w, \delta^2)$ 为该模型的参数

线性回归也能为非线性关系建模。我们将$x$换为非线性输入$\phi(x)$，例如

$$p(y|x, \theta) = \mathcal{N}(y|w^T\phi(x), \delta^2)$$

这被称为基函数拓展(basis function expansion)

令 $\phi(x) = [1, x, x^2, ..., x^d]$，我们将得到一个多项式回归(polynomial regression)

实际上，很多机器学习方法，例如支持向量机，神经网络，分类器，回归树等，都可看作是对数据的近似基函数的不同的看待方式

### 1.4.6 逻辑回归

我们可以通过改两个地方，让线性回归适用于二进制分类问题

第一，我们用伯努利分布(Bernoulli distribution)代替高斯分布，其更适用于二进制问题，$y \in {0, 1}$

$$p(y|x, w) = Ber(y|\mu(x))$$

其中 $\mu(x) = \Xi[y|x] = p(y=1|x)$

第二，我们用sigmoid函数(称为logistic或logit函数)限制 $0 \le \mu(x) \le 1$

$$\mu(x) = sigm(w^Tx)$$

sigmoid函数定义

$$sigm(\eta) \triangleq \frac{1}{1+exp(-\eta)} = \frac{e^\eta}{e^\eta+1} $$

sigmod函数为S-型函数，也被称为挤压函数(squashing function)，能够很好的将输入变换到$[0,1]$上，友好的表示概率

结合以上两步，我们得到

$$p(y|x, w) = Ber(y|sigm(w^TX))$$

这叫逻辑回归(logistic regression)，用于$0-1$分类

### 1.4.8 模型选择

如何决定选择哪种模型呢？一种直观的方式是计算错误分类率(misclassification rate)

$$err(f, D) = \frac{1}{N}\sum_{i=1}^{N}\prod(f(x_i) \ne y_i))$$

注意这里我们关注的是广义误差(generalization error)，需要在测试集而非训练集上验证。通常修改我们的参数，我们会得到一个U-型曲线(U-shaped curve)，两头分别为过拟合(overfits)和欠拟合(underfits)的结果，我们选取中间的参数。

我们不能依靠测试集来选去模型，否则我们将得到不切实际的乐观结果，这是机器学习中的金科玉律。但是我们可以选择把训练集划分为两部分，第二部分称为验证集(validation set)，然后通过它来选择最适合模型的参数。

通常我们用80%的数据训练，20%的数据验证。但是如果样本太小，我们将没有足够的数据用于训练。一个常用的方法是交叉验证(cross validation (CV))，我们把训练集分为K份，每次选一份为验证集，在其他份上进行训练。最后计算误差时，取这K次训练的均值。通常$K=5$。如果$K=N$，则称为留一交叉验证(leave-one out cross validation, LOOCV)

### 1.4.9 没有免费午餐定理

没有一种模型适合所有场景，因此我们需要针对不同的场景选择不同的模型；对于同一模型，需要选取不同的算法进行训练，在速度，精确度，复杂度之前做取舍。





