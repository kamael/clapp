### 1.4.1 参数 vs. 无参数模型

本书中，我们将使用 ![p(y|x)](http://latex.codecogs.com/gif.latex?p%28y%7Cx%29) 或 ![p(x)](http://latex.codecogs.com/gif.latex?p%28x%29) 这样形式的概率模型，取决于是有监督学习还是无监督学习。

有很多方法可用于定义这类模型。其中有一个最重要的区别：是否有参数(parametric model or non-parametric model)，下面将进行解释。

### 1.4.2 简单的无参模型：K邻聚类

K邻聚类(K nearest neighbor (KNN) classifier)表示对于输入![x](http://latex.codecogs.com/gif.latex?x)，在离他最近的点中，哪一类的点最多。

![p(y=c|x, D, K) = \frac{1}{K}\sum_{i\in N_K(x, D)}{\prod(y_i=c)}](http://latex.codecogs.com/gif.latex?p%28y%3Dc%7Cx%2C%20D%2C%20K%29%20%3D%20%5Cfrac%7B1%7D%7BK%7D%5Csum_%7Bi%5Cin%20N_K%28x%2C%20D%29%7D%7B%5Cprod%28y_i%3Dc%29%7D)

其中![N_K(x, D)](http://latex.codecogs.com/gif.latex?N_K%28x%2C%20D%29)表示在D上，离![x](http://latex.codecogs.com/gif.latex?x)第![K](http://latex.codecogs.com/gif.latex?K)近的点集；![K](http://latex.codecogs.com/gif.latex?K)值由模型指定，![\prod(e)](http://latex.codecogs.com/gif.latex?%5Cprod%28e%29) 为如下指示函数：

![
y = \left\{ \begin{array}{ll}
1 & \textrm{if ![e](http://latex.codecogs.com/gif.latex?e) is true}\\
0 & \textrm{if ![e](http://latex.codecogs.com/gif.latex?e) is false}\\
\end{array} \right.
](http://latex.codecogs.com/gif.latex?%0Ay%20%3D%20%5Cleft%5C%7B%20%5Cbegin%7Barray%7D%7Bll%7D%0A1%20%26%20%5Ctextrm%7Bif%20%24e%24%20is%20true%7D%5C%5C%0A0%20%26%20%5Ctextrm%7Bif%20%24e%24%20is%20false%7D%5C%5C%0A%5Cend%7Barray%7D%20%5Cright.%0A)

这是一个基于内存或实例学习(memory-based learning or instance-based learning)的例子。最常用的距离测量方式为欧几里得距离。

当 ![K=1](http://latex.codecogs.com/gif.latex?K%3D1) 的时候，其将退化为沃罗诺伊镶嵌(Voronoi tessellation)，距离输入点最近的点的标签即为该点的标签。

如果数据集足够好，当 ![N\to\infty](http://latex.codecogs.com/gif.latex?N%5Cto%5Cinfty) 时，K邻聚类可以得到很好的结果。

但是K邻聚类不适合处理高纬问题，纬度灾难(Curse of dimensionality
)问题会导致维度上升时，样本空间过小，无法满足分类的需求

### 1.4.4 参数模型

一个解决维度灾难的办法是假定数据分布的规律，这些假定称之为归纳偏置(inductive bias)，常常以参数模型(parametric model)的形式出现。后文将深入解释。

### 1.4.5 线性回归

一个用的很广的模型称为线性回归(linear regression)

![y(x) = w^T + \epsilon + \sum_{j=1}^{D}w_jx_j + \epsilon ](http://latex.codecogs.com/gif.latex?y%28x%29%20%3D%20w%5ET%20%2B%20%5Cepsilon%20%2B%20%5Csum_%7Bj%3D1%7D%5E%7BD%7Dw_jx_j%20%2B%20%5Cepsilon%20)

![w^T](http://latex.codecogs.com/gif.latex?w%5ET)表示权重![w](http://latex.codecogs.com/gif.latex?w)(weight vector)和输入向量![x](http://latex.codecogs.com/gif.latex?x)(input vector)的标量积(scalar product)，![\epsilon](http://latex.codecogs.com/gif.latex?%5Cepsilon)表示线性预测值和真实值之间的残差(residual error)

我们通常假定![\epsilon](http://latex.codecogs.com/gif.latex?%5Cepsilon)符合高斯或平凡(Gaussian or normal)分布，即 ![\epsilon \sim \mathcal{N}(\mu, \delta^2)](http://latex.codecogs.com/gif.latex?%5Cepsilon%20%5Csim%20%5Cmathcal%7BN%7D%28%5Cmu%2C%20%5Cdelta%5E2%29)，其中![\mu](http://latex.codecogs.com/gif.latex?%5Cmu)为均值(mean)，![\delta^2](http://latex.codecogs.com/gif.latex?%5Cdelta%5E2)为方差(variance)。我们将其画出，将得到一个著名的钟形曲线(bell curve)

为了更加紧密的反应线性回归和高斯分布之间的关系，我们用如下方式表示这个模型

![p(y|x, \theta) = \mathcal{N}(y|\mu(x), \delta^2(x))](http://latex.codecogs.com/gif.latex?p%28y%7Cx%2C%20%5Ctheta%29%20%3D%20%5Cmathcal%7BN%7D%28y%7C%5Cmu%28x%29%2C%20%5Cdelta%5E2%28x%29%29)

这清楚的反应该模型是条件概率密度模型

最简单的场景下，我们假定![\mu](http://latex.codecogs.com/gif.latex?%5Cmu)是x的线性函数，即 ![\mu = w^Tx](http://latex.codecogs.com/gif.latex?%5Cmu%20%3D%20w%5ETx)；假定噪声是常量，即 ![\delta^2(x) = \delta^2](http://latex.codecogs.com/gif.latex?%5Cdelta%5E2%28x%29%20%3D%20%5Cdelta%5E2)。在该场景下，![\theta = (w, \delta^2)](http://latex.codecogs.com/gif.latex?%5Ctheta%20%3D%20%28w%2C%20%5Cdelta%5E2%29) 为该模型的参数

线性回归也能为非线性关系建模。我们将![x](http://latex.codecogs.com/gif.latex?x)换为非线性输入![\phi(x)](http://latex.codecogs.com/gif.latex?%5Cphi%28x%29)，例如

![p(y|x, \theta) = \mathcal{N}(y|w^T\phi(x), \delta^2)](http://latex.codecogs.com/gif.latex?p%28y%7Cx%2C%20%5Ctheta%29%20%3D%20%5Cmathcal%7BN%7D%28y%7Cw%5ET%5Cphi%28x%29%2C%20%5Cdelta%5E2%29)

这被称为基函数拓展(basis function expansion)

令 ![\phi(x) = \[1, x, x^2, ..., x^d\]](http://latex.codecogs.com/gif.latex?%5Cphi%28x%29%20%3D%20%5B1%2C%20x%2C%20x%5E2%2C%20...%2C%20x%5Ed%5D)，我们将得到一个多项式回归(polynomial regression)

实际上，很多机器学习方法，例如支持向量机，神经网络，分类器，回归树等，都可看作是对数据的近似基函数的不同的看待方式

### 1.4.6 逻辑回归

我们可以通过改两个地方，让线性回归适用于二进制分类问题

第一，我们用伯努利分布(Bernoulli distribution)代替高斯分布，其更适用于二进制问题，![y \in {0, 1}](http://latex.codecogs.com/gif.latex?y%20%5Cin%20%7B0%2C%201%7D)

![p(y|x, w) = Ber(y|\mu(x))](http://latex.codecogs.com/gif.latex?p%28y%7Cx%2C%20w%29%20%3D%20Ber%28y%7C%5Cmu%28x%29%29)

其中 ![\mu(x) = \Xi\[y|x\] = p(y=1|x)](http://latex.codecogs.com/gif.latex?%5Cmu%28x%29%20%3D%20%5CXi%5By%7Cx%5D%20%3D%20p%28y%3D1%7Cx%29)

第二，我们用sigmoid函数(称为logistic或logit函数)限制 ![0 \le \mu(x) \le 1](http://latex.codecogs.com/gif.latex?0%20%5Cle%20%5Cmu%28x%29%20%5Cle%201)

![\mu(x) = sigm(w^Tx)](http://latex.codecogs.com/gif.latex?%5Cmu%28x%29%20%3D%20sigm%28w%5ETx%29)

sigmoid函数定义

![sigm(\eta) \triangleq \frac{1}{1+exp(-\eta)} = \frac{e^\eta}{e^\eta+1} ](http://latex.codecogs.com/gif.latex?sigm%28%5Ceta%29%20%5Ctriangleq%20%5Cfrac%7B1%7D%7B1%2Bexp%28-%5Ceta%29%7D%20%3D%20%5Cfrac%7Be%5E%5Ceta%7D%7Be%5E%5Ceta%2B1%7D%20)

sigmod函数为S-型函数，也被称为挤压函数(squashing function)，能够很好的将输入变换到![\[0,1\]](http://latex.codecogs.com/gif.latex?%5B0%2C1%5D)上，友好的表示概率

结合以上两步，我们得到

![p(y|x, w) = Ber(y|sigm(w^TX))](http://latex.codecogs.com/gif.latex?p%28y%7Cx%2C%20w%29%20%3D%20Ber%28y%7Csigm%28w%5ETX%29%29)

这叫逻辑回归(logistic regression)，用于![0-1](http://latex.codecogs.com/gif.latex?0-1)分类

### 1.4.8 模型选择

如何决定选择哪种模型呢？一种直观的方式是计算错误分类率(misclassification rate)

![err(f, D) = \frac{1}{N}\sum_{i=1}^{N}\prod(f(x_i) \ne y_i))](http://latex.codecogs.com/gif.latex?err%28f%2C%20D%29%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cprod%28f%28x_i%29%20%5Cne%20y_i%29%29)

注意这里我们关注的是广义误差(generalization error)，需要在测试集而非训练集上验证。通常修改我们的参数，我们会得到一个U-型曲线(U-shaped curve)，两头分别为过拟合(overfits)和欠拟合(underfits)的结果，我们选取中间的参数。

我们不能依靠测试集来选去模型，否则我们将得到不切实际的乐观结果，这是机器学习中的金科玉律。但是我们可以选择把训练集划分为两部分，第二部分称为验证集(validation set)，然后通过它来选择最适合模型的参数。

通常我们用80%的数据训练，20%的数据验证。但是如果样本太小，我们将没有足够的数据用于训练。一个常用的方法是交叉验证(cross validation (CV))，我们把训练集分为K份，每次选一份为验证集，在其他份上进行训练。最后计算误差时，取这K次训练的均值。通常![K=5](http://latex.codecogs.com/gif.latex?K%3D5)。如果![K=N](http://latex.codecogs.com/gif.latex?K%3DN)，则称为留一交叉验证(leave-one out cross validation, LOOCV)

### 1.4.9 没有免费午餐定理

没有一种模型适合所有场景，因此我们需要针对不同的场景选择不同的模型；对于同一模型，需要选取不同的算法进行训练，在速度，精确度，复杂度之前做取舍。





